# Databricks notebook source
from databricks.sdk import WorkspaceClient
import databricks.sdk.service.catalog as c
import re
from langchain.text_splitter import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter
from transformers import AutoTokenizer, OpenAIGPTTokenizer

client = WorkspaceClient()

catalogs = client.catalogs.list()
catalog_names = [catalog.name for catalog in catalogs]

# Specify the catalog and database
catalog = 'shaip_physician_dictation_data_family_medicine'
db = "physiciandictationspeech(&transcript)"

# Read the JSON file from the specified catalog and database
json_df = spark.read.json(f"/Volumes/shaip_physician_dictation_data_family_medicine/physiciandictationspeech(&transcript)/familymedicine/en_US_145512_20200618.json", multiLine=True)

# Display the DataFrame
transcription_data_df = json_df.select("value.segments.transcriptionData")

content_list = [segment['content'] for row in transcription_data_df.collect() for segment in row['transcriptionData']]
tokenizer = AutoTokenizer.from_pretrained("gpt2")

def split_content_on_h2(content_list, min_chunk_size=20, max_chunk_size=500, text_splitter=RecursiveCharacterTextSplitter()):
    if not content_list:
        return []
    
    chunks = []
    previous_chunk = ""
    for content in content_list:
        # Process each content item
        if len(tokenizer.encode(previous_chunk + content)) <= max_chunk_size / 2:
            previous_chunk += content + "\n"
        else:
            chunks.extend(text_splitter.split_text(previous_chunk.strip()))
            previous_chunk = content + "\n"
    
    if previous_chunk:
        chunks.extend(text_splitter.split_text(previous_chunk.strip()))
    
    return [c for c in chunks if len(tokenizer.encode(c)) > min_chunk_size]

# Use the modified function
combined_content = spark.createDataFrame(split_content_on_h2(content_list), schema=['content'])

if not spark.catalog.tableExists("raw_documentation") or spark.table("raw_documentation").isEmpty():
    # Download Databricks documentation to a DataFrame (see _resources/00-init for more details)
    # Save them as a raw_documentation table
    combined_content.write.mode('overwrite').saveAsTable("raw_documentation")

display(spark.table("raw_documentation").limit(2))

# COMMAND ----------

# MAGIC %sql
# MAGIC --Note that we need to enable Change Data Feed on the table to create the index
# MAGIC CREATE TABLE IF NOT EXISTS transcriptionData (
# MAGIC   id BIGINT GENERATED BY DEFAULT AS IDENTITY,
# MAGIC   content STRING
# MAGIC ) TBLPROPERTIES (delta.enableChangeDataFeed = true); 

# COMMAND ----------

from pyspark.sql.functions import explode, col
from pyspark.sql.types import ArrayType, StringType
import pyspark.sql.functions as F

# Define the parse_and_split function
def parse_and_split(content):
    # Example implementation: split the content by spaces
    return content.split(".")

# Register the function as a UDF
parse_and_split_udf = F.udf(parse_and_split, ArrayType(StringType()))

(spark.table("raw_documentation")
      .filter('content is not null')  # Corrected column name
      .repartition(30)
      .withColumn('content', explode(parse_and_split_udf(col('content'))))
      .write.mode('overwrite').saveAsTable("transcriptionData"))

display(spark.table("transcriptionData"))

from databricks.vector_search.client import VectorSearchClient
vsc = VectorSearchClient()

# Define the endpoint_exists function
def endpoint_exists(client, endpoint_name):
    endpoints = client.list_endpoints() if isinstance(client.list_endpoints(), list) else []
    return any(endpoint['name'] == endpoint_name for endpoint in endpoints)

VECTOR_SEARCH_ENDPOINT_NAME = "virtualphysicalchatbot"

if not endpoint_exists(vsc, VECTOR_SEARCH_ENDPOINT_NAME):
    vsc.create_endpoint(name=VECTOR_SEARCH_ENDPOINT_NAME, endpoint_type="STANDARD")

def wait_for_vs_endpoint_to_be_ready(client, endpoint_name):    # Implementation to wait for the endpoint to be ready
    pass

wait_for_vs_endpoint_to_be_ready(vsc, VECTOR_SEARCH_ENDPOINT_NAME)

print(f"Endpoint named {VECTOR_SEARCH_ENDPOINT_NAME} is ready.")

# COMMAND ----------

# Ensure catalog, db, and table names are correctly formatted
catalog = "workspace"
db = "default"
table = "transcriptionData"
#workspace.default.transcriptionDataIndex_v2
# The table we'd like to index
source_table_fullname = f"{catalog}.{db}.{table}"
# Use a unique name for the index
vs_index_fullname = f"{catalog}.{db}.{table}Index_v2"  # Changed to ensure uniqueness

if not index_exists(vsc, VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname):
  print(f"Creating index {vs_index_fullname} on endpoint {VECTOR_SEARCH_ENDPOINT_NAME}...")
  try:
    vsc.create_delta_sync_index(
      endpoint_name=VECTOR_SEARCH_ENDPOINT_NAME,
      index_name=vs_index_fullname,
      source_table_name=source_table_fullname,
      pipeline_type="TRIGGERED",
      primary_key="id",
      embedding_source_column='content',
      embedding_model_endpoint_name='databricks-gte-large-en'
    )
  except Exception as e:
    raise e
  wait_for_index_to_be_ready(vsc, VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname)
else:
  wait_for_index_to_be_ready(vsc, VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname)
  vsc.get_index(VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname).sync()

print(f"index {vs_index_fullname} on table {source_table_fullname} is ready")